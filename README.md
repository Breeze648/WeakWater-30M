# WeakWater-30M
本项目从零开始构建并优化了一个千万参数级别的大规模预训练语言模型，涵盖预训练、有监督微调（SFT）和R1推理蒸馏三个阶段。项目采用自定义Transformer架构（包括RMSNorm、分组注意力、多Query机制、SwiGLU激活和RoPE位置编码），实现高效的长文本处理和自回归生成。同时，开发了基于BBPE编码的分词器，针对中文进行了优化，构建了轻量级词表。预训练阶段通过混合精度训练、梯度累积和Cosine Annealing学习率调度，实现全流程训练；微调阶段则利用开源SFT数据集，通过特殊损失掩码优化指令遵循；R1推理蒸馏阶段采用Deepseek-R1数据及损失加权策略，赋予模型逐步推理（慢思考）的能力，显著提升复杂任务的回答质量。
